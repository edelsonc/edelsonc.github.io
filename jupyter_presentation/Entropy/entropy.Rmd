---
title: "Entropy for Data Science"
author: Charlie Edelson, Caleb Dowdy, Chris Leonard,  Nicole Navarro, Aaron Niskin, Lance Price
date: "12/4/2017"
output:
    beamer_presentation: 
        theme: "EastLansing"
        color: "seahorse"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Outline
* History
    - Statistical Mechanics
* Shannon Entropy
    - Uniform Distribution
    - Normal Distribution
* Tsallis Entropy

# History

## Statistical Mechanics
Consider a box with $N$ particles of a monatomic gas

\begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{images/particles.png}
\end{figure}

How would you model this?

## Statistical Mechanics - State Variables?

* We can talk about state variables: $P$, $T$, $N$, and $V$ \pause 

    - Ideal Gas Law

\begin{equation}
    PV = nRT    
\end{equation} \pause

* Characterize System Behaviors

## Statistical Mechanics - Ensemble Statistics
\begin{columns}
    \begin{column}{0.4\textwidth}
       \begin{figure}
                \includegraphics[width=0.5\textwidth]{images/newton-particles.png}
        \end{figure}
    \end{column}
    \begin{column}{0.6\textwidth}
        Assume each particle obeys Newton's Law
        \begin{itemize}
            \item{ $v_{0}$ and $x_0$ determines system}
            \item{Impractical for large $N$}
        \end{itemize}
        \pause
        \vspace{0.2cm}
        James Maxwell's Kinetic Theory of Gases
        \begin{itemize}
            \item{Consider Ensamble Statistics}
        \end{itemize} \vspace{0.2cm}
        \begin{equation}
            PV = \frac{Nm\bar{v^2}}{3}
        \end{equation}
    \end{column}
\end{columns}

## Statistical Mechanics Entropy
\begin{center}
    Average Behaviour $\rightarrow$ Macroscopic Properties
\end{center}
\pause
Ludwig Boltzman's statistical mechanical entropy (1877)
\begin{equation}
    S = k_b\ln(\Omega)
\end{equation}

$\Omega$ is the multiplicity of a given macrostate

## Macrostates, Microstates, and Multiplicity
Consider non-interacting paramagnet with 3 dipoles
\begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{images/spins.png}
\end{figure} \pause

\vspace{-.2cm}

* Macrostate -  2 Up, 1 Down \pause

* Microstate - $\uparrow$, $\downarrow$, $\uparrow$ \pause

* $\Omega = {{N}\choose{N_\uparrow}} = {{3}\choose{2}}$

## Entropy of 100 Dipole Paramagnet

```{r, echo=FALSE}
N <- 100
kb <- 1.38064852e-23
ups <- c(0:N)
multiplicity <- choose(N, ups)
entropy <- kb * log(multiplicity)
plot(ups, multiplicity, xlab = expression(N[symbol("\255")]), ylab = "Entropy")
```

## Interpretation
Features of paramagnet entropy

* Minimum at 0 and 100 $\rightarrow$ 1 microstate each
* Maximum at 50 $\rightarrow \space 10^{29}$ microstates! \pause

Measure of ``mixed-up-ness" of a physical system

* Higher entropy $\rightarrow$ more mixing (randomness)
* Lower entropy $\rightarrow$ less mixing

# Shannon Entropy

## Telephone Line Information Loss

Claude Shannon at Bell Telephone (1939)

* Quantify "lost information" in phone-line signals
* "Information Uncertainty"

\begin{equation}
H = - K \sum_{i=1}^k p(i)\log(p(i))
\end{equation}

Difficult time naming $H$...

## Naming "Information Uncertainty"
...until he visited John von Neumann

> My greatest concern was what to call it. I thought of calling it ‘information’, but the word was overly used, so I decided to call it ‘uncertainty’. When I discussed it with John von Neumann, he had a better idea. Von Neumann told me, ‘You should call it entropy, for two reasons: In the first place your uncertainty function has been used in statistical mechanics under that name, so it already has a name. In the second place, and more important, nobody knows what entropy really is, so in a debate you will always have the advantage.

## Shannon Entropy

Same properties as statistical mechanic entropy

* Low entropy $\rightarrow$ low randomness
* High entropy $\rightarrow$ high randomness \pause

Directely related to information content of random variable X

* Unpredicatbility of state
* Average information content \pause

\begin{equation}
    \text{H}(X) = \text{E}[-\ln(X)]
\end{equation}

## Shannon Entropy - Uniform Distribution

\begin{figure}
        \includegraphics[width=0.5\textwidth]{images/uniform.png}
\end{figure}

\vspace{-.2cm}

* Maximum Entropy
    - $\text{H}(X) = \ln(N)$
* Boltzman Statisical Mechanics Entropy

